{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document Embeddings (Ollama Compatible)\n",
    "This notebook creates embeddings that match Ollama's deepseek model dimensions (3584)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install sentence-transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model that outputs 3072 dimensions (close to Ollama's 3584)\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Function to pad embeddings to match Ollama's dimension\n",
    "def pad_embedding(embedding, target_size=3584):\n",
    "    current_size = len(embedding)\n",
    "    if current_size >= target_size:\n",
    "        return embedding[:target_size]\n",
    "    padding = [0.0] * (target_size - current_size)\n",
    "    return embedding + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Upload your chunks.json file here\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "filename = list(uploaded.keys())[0]\n",
    "\n",
    "# Load chunks\n",
    "with open(filename, 'r') as f:\n",
    "    chunks = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Process chunks in batches\n",
    "embeddings_data = []\n",
    "batch_size = 32  # Larger batch size for GPU efficiency\n",
    "\n",
    "for i in tqdm(range(0, len(chunks), batch_size)):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "    # Get embeddings for entire batch at once\n",
    "    batch_embeddings = model.encode([chunk for chunk in batch], convert_to_tensor=True)\n",
    "    \n",
    "    for j, embedding in enumerate(batch_embeddings):\n",
    "        # Convert to list and pad to match Ollama's dimensions\n",
    "        padded_embedding = pad_embedding(embedding.cpu().numpy().tolist())\n",
    "        embeddings_data.append({\n",
    "            \"id\": i + j,\n",
    "            \"content\": batch[j],\n",
    "            \"embedding\": padded_embedding\n",
    "        })\n",
    "\n",
    "# Save embeddings\n",
    "output_filename = filename.replace('_chunks.json', '_embeddings.json')\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(embeddings_data, f)\n",
    "\n",
    "print(f\"\\nSaved embeddings for {len(embeddings_data)} chunks\")\n",
    "\n",
    "# Download the embeddings file\n",
    "files.download(output_filename)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Create Document Embeddings (Ollama Compatible)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
