{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Trading Model (Stability Optimized)\n",
    "\n",
    "This notebook implements a deep learning model for trading using:\n",
    "\n",
    "- LSTM + CNN + Attention architecture\n",
    "- Multi-symbol training with market regime awareness\n",
    "- Numerical stability improvements\n",
    "- Technical indicators and market context\n",
    "- Custom loss functions with stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow pandas numpy scikit-learn yfinance plotly ta\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ta\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Dense, Dropout, BatchNormalization,\n",
    "    Conv1D, LayerNormalization, Activation, Add, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau, Callback\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import yfinance as yf\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Use float32 for better stability\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "# Configure memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(\"\\nGPU Configuration:\")\n",
    "print(\"Number of GPUs:\", len(gpus))\n",
    "print(\"Data Type:\", tf.keras.backend.floatx())\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Verify GPU is being used\n",
    "print(\"\\nGPU Available:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU Device:\", tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "def calculate_indicators(df):\n",
    "    \"\"\"Calculate technical indicators using the ta library.\"\"\"\n",
    "    # Initialize indicators\n",
    "    bb_indicator = ta.volatility.BollingerBands(df['Close'])\n",
    "    \n",
    "    # Moving averages\n",
    "    df['sma_20'] = ta.trend.sma_indicator(df['Close'], window=20)\n",
    "    df['sma_50'] = ta.trend.sma_indicator(df['Close'], window=50)\n",
    "    df['sma_200'] = ta.trend.sma_indicator(df['Close'], window=200)\n",
    "    \n",
    "    df['ema_10'] = ta.trend.ema_indicator(df['Close'], window=10)\n",
    "    df['ema_20'] = ta.trend.ema_indicator(df['Close'], window=20)\n",
    "    df['ema_50'] = ta.trend.ema_indicator(df['Close'], window=50)\n",
    "    \n",
    "    # Momentum\n",
    "    df['rsi'] = ta.momentum.rsi(df['Close'], window=14)\n",
    "    df['macd'] = ta.trend.macd_diff(df['Close'])\n",
    "    df['mom_1d'] = df['Close'].pct_change(1)\n",
    "    df['mom_5d'] = df['Close'].pct_change(5)\n",
    "    df['mom_10d'] = df['Close'].pct_change(10)\n",
    "    df['mom_20d'] = df['Close'].pct_change(20)\n",
    "    \n",
    "    # Volatility\n",
    "    df['atr'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'])\n",
    "    df['atr_pct'] = df['atr'] / df['Close']\n",
    "    df['bb_high'] = bb_indicator.bollinger_hband()\n",
    "    df['bb_mid'] = bb_indicator.bollinger_mavg()\n",
    "    df['bb_low'] = bb_indicator.bollinger_lband()\n",
    "    df['bb_width'] = (df['bb_high'] - df['bb_low']) / df['bb_mid']\n",
    "    \n",
    "    # Volume\n",
    "    df['volume_ma_20'] = ta.trend.sma_indicator(df['Volume'], window=20)\n",
    "    df['volume_ma_50'] = ta.trend.sma_indicator(df['Volume'], window=50)\n",
    "    df['volume_ratio'] = df['Volume'] / df['volume_ma_20']\n",
    "    df['volume_trend'] = df['Volume'].pct_change(5)\n",
    "    \n",
    "    # Price patterns\n",
    "    df['high_low_range'] = (df['High'] - df['Low']) / df['Close']\n",
    "    df['close_to_high'] = (df['High'] - df['Close']) / df['Close']\n",
    "    df['close_to_low'] = (df['Close'] - df['Low']) / df['Close']\n",
    "    \n",
    "    # Trend strength\n",
    "    df['trend_strength'] = np.where(\n",
    "        df['Close'] > df['sma_50'],\n",
    "        (df['Close'] - df['sma_50']) / df['sma_50'],\n",
    "        -(df['sma_50'] - df['Close']) / df['sma_50']\n",
    "    )\n",
    "    \n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "def calculate_cross_asset_features(tech_data):\n",
    "    \"\"\"Calculate cross-asset relationships between tech stocks.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Calculate returns for correlation\n",
    "    returns = {}\n",
    "    for symbol in tech_data:\n",
    "        returns[symbol] = tech_data[symbol]['Close'].pct_change()\n",
    "    \n",
    "    # Rolling correlations and relative strength\n",
    "    for symbol in tech_data:\n",
    "        # Initialize features for this symbol\n",
    "        features[symbol] = pd.DataFrame(index=tech_data[symbol].index)\n",
    "        \n",
    "        # Calculate correlations with other stocks\n",
    "        for other in tech_data:\n",
    "            if other != symbol:\n",
    "                # Rolling correlation\n",
    "                corr = returns[symbol].rolling(20).corr(returns[other])\n",
    "                features[symbol][f'corr_{other}'] = corr\n",
    "                \n",
    "                # Relative strength\n",
    "                rel_strength = (tech_data[symbol]['Close'] / \n",
    "                               tech_data[symbol]['Close'].shift(20)) / \\\n",
    "                              (tech_data[other]['Close'] / \n",
    "                               tech_data[other]['Close'].shift(20))\n",
    "                features[symbol][f'rel_strength_{other}'] = rel_strength\n",
    "                \n",
    "                # Relative volume\n",
    "                rel_volume = (tech_data[symbol]['Volume'] / \n",
    "                             tech_data[symbol]['volume_ma_20']) / \\\n",
    "                            (tech_data[other]['Volume'] / \n",
    "                             tech_data[other]['volume_ma_20'])\n",
    "                features[symbol][f'rel_volume_{other}'] = rel_volume\n",
    "        \n",
    "        # Sector-wide features\n",
    "        tech_returns = pd.DataFrame([returns[s] for s in tech_data]).T\n",
    "        \n",
    "        # Stock's return vs sector average\n",
    "        sector_avg_return = tech_returns.mean(axis=1)\n",
    "        features[symbol]['sector_relative_return'] = \\\n",
    "            returns[symbol] - sector_avg_return\n",
    "        \n",
    "        # Stock's momentum vs sector average\n",
    "        stock_mom = tech_data[symbol]['mom_20d']\n",
    "        sector_mom = pd.DataFrame([tech_data[s]['mom_20d'] \n",
    "                                  for s in tech_data]).T.mean(axis=1)\n",
    "        features[symbol]['sector_relative_momentum'] = \\\n",
    "            stock_mom - sector_mom\n",
    "        \n",
    "        # Stock's volatility vs sector average\n",
    "        stock_vol = tech_data[symbol]['atr_pct']\n",
    "        sector_vol = pd.DataFrame([tech_data[s]['atr_pct'] \n",
    "                                  for s in tech_data]).T.mean(axis=1)\n",
    "        features[symbol]['sector_relative_volatility'] = \\\n",
    "            stock_vol - sector_vol\n",
    "        \n",
    "        # Number of sector stocks above their SMAs\n",
    "        sma_signals = pd.DataFrame([\n",
    "            tech_data[s]['Close'] > tech_data[s]['sma_50']\n",
    "            for s in tech_data\n",
    "        ]).T\n",
    "        features[symbol]['sector_sma_strength'] = \\\n",
    "            sma_signals.sum(axis=1) / len(tech_data)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def calculate_market_regimes(data):\n",
    "    \"\"\"Calculate market regime features from ETF data.\"\"\"\n",
    "    # Get market context data\n",
    "    spy_data = data['SPY'].copy()\n",
    "    qqq_data = data['QQQ'].copy()\n",
    "    uvxy_data = data['UVXY'].copy()\n",
    "    sqqq_data = data['SQQQ'].copy()\n",
    "    tlt_data = data['TLT'].copy()\n",
    "    \n",
    "    # Market trend regime\n",
    "    spy_data['market_trend'] = spy_data['Close'].pct_change(20)\n",
    "    spy_data['market_regime'] = pd.qcut(\n",
    "        spy_data['market_trend'],\n",
    "        q=5,\n",
    "        labels=['strong_down', 'down', 'neutral', 'up', 'strong_up']\n",
    "    )\n",
    "    \n",
    "    # Tech sector regime\n",
    "    qqq_data['tech_trend'] = qqq_data['Close'].pct_change(20)\n",
    "    qqq_data['tech_regime'] = pd.qcut(\n",
    "        qqq_data['tech_trend'],\n",
    "        q=5,\n",
    "        labels=['tech_strong_down', 'tech_down', 'tech_neutral', 'tech_up', 'tech_strong_up']\n",
    "    )\n",
    "    \n",
    "    # Volatility regime\n",
    "    uvxy_data['volatility'] = uvxy_data['Close'].pct_change(5)\n",
    "    uvxy_data['volatility_regime'] = pd.qcut(\n",
    "        uvxy_data['volatility'],\n",
    "        q=5,\n",
    "        labels=['very_low', 'low', 'normal', 'high', 'very_high']\n",
    "    )\n",
    "    \n",
    "    # Bear market signals\n",
    "    sqqq_data['bear_signal'] = sqqq_data['Close'].pct_change(10)\n",
    "    sqqq_data['bear_regime'] = pd.qcut(\n",
    "        sqqq_data['bear_signal'],\n",
    "        q=5,\n",
    "        labels=['strong_bull', 'bull', 'neutral', 'bear', 'strong_bear']\n",
    "    )\n",
    "    \n",
    "    # Interest rate regime\n",
    "    tlt_data['rates_trend'] = tlt_data['Close'].pct_change(20)\n",
    "    tlt_data['rates_regime'] = pd.qcut(\n",
    "        tlt_data['rates_trend'],\n",
    "        q=5,\n",
    "        labels=['rates_up_strong', 'rates_up', 'rates_neutral', 'rates_down', 'rates_down_strong']\n",
    "    )\n",
    "    \n",
    "    # Additional market context features\n",
    "    context = {\n",
    "        'market_regime': spy_data['market_regime'],\n",
    "        'tech_regime': qqq_data['tech_regime'],\n",
    "        'volatility_regime': uvxy_data['volatility_regime'],\n",
    "        'bear_regime': sqqq_data['bear_regime'],\n",
    "        'rates_regime': tlt_data['rates_regime'],\n",
    "        \n",
    "        # Relative strength between markets\n",
    "        'tech_vs_spy': (qqq_data['Close'] / qqq_data['Close'].shift(20)) / \\\n",
    "                      (spy_data['Close'] / spy_data['Close'].shift(20)),\n",
    "                      \n",
    "        # Volatility trends\n",
    "        'vol_trend': uvxy_data['Close'].pct_change(5).rolling(5).mean(),\n",
    "        \n",
    "        # Market breadth\n",
    "        'market_momentum': spy_data['mom_20d'],\n",
    "        'tech_momentum': qqq_data['mom_20d'],\n",
    "        \n",
    "        # Risk measures\n",
    "        'market_risk': spy_data['atr_pct'].rolling(10).mean(),\n",
    "        'tech_risk': qqq_data['atr_pct'].rolling(10).mean()\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def download_and_prepare_data(tech_stocks, market_etfs, start_date='2023-01-01'):\n",
    "    \"\"\"Download and prepare data for multiple symbols with cross-asset relationships.\"\"\"\n",
    "    all_data = {}\n",
    "    all_symbols = tech_stocks + market_etfs\n",
    "    \n",
    "    # Download and process all data\n",
    "    for symbol in all_symbols:\n",
    "        print(f\"Processing {symbol}...\")\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        df = ticker.history(start=start_date)\n",
    "        df = calculate_indicators(df)\n",
    "        all_data[symbol] = df\n",
    "    \n",
    "    # Calculate cross-asset features for tech stocks\n",
    "    tech_data = {symbol: all_data[symbol] for symbol in tech_stocks}\n",
    "    cross_features = calculate_cross_asset_features(tech_data)\n",
    "    \n",
    "    # Calculate market regimes and context\n",
    "    market_context = calculate_market_regimes(all_data)\n",
    "    \n",
    "    # Combine all features\n",
    "    processed_data = []\n",
    "    for symbol in all_symbols:\n",
    "        df = all_data[symbol].copy()\n",
    "        \n",
    "        # Add instrument type\n",
    "        df['instrument_type'] = 'stock' if symbol in tech_stocks else 'etf'\n",
    "        df['symbol'] = symbol\n",
    "        \n",
    "        # Add cross-asset features for tech stocks\n",
    "        if symbol in tech_stocks:\n",
    "            df = pd.concat([df, cross_features[symbol]], axis=1)\n",
    "        \n",
    "        # Add market context\n",
    "        df = pd.concat([df, market_context], axis=1)\n",
    "        \n",
    "        processed_data.append(df)\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_data = pd.concat(processed_data, axis=0)\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "# Define instruments\n",
    "tech_stocks = ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'AMD', 'META']\n",
    "market_etfs = ['SPY', 'QQQ', 'UVXY', 'SQQQ', 'TLT']\n",
    "\n",
    "# Download and prepare data\n",
    "data = download_and_prepare_data(tech_stocks, market_etfs)\n",
    "\n",
    "# Plot sample data\n",
    "fig = go.Figure()\n",
    "for symbol in tech_stocks:\n",
    "    symbol_data = data[data['symbol'] == symbol]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=symbol_data.index,\n",
    "        y=symbol_data['Close'],\n",
    "        name=symbol\n",
    "    ))\n",
    "fig.update_layout(title='Tech Stock Prices', xaxis_title='Date', yaxis_title='Price')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "def prepare_sequences(data, seq_length=60):\n",
    "    \"\"\"Prepare sequences with stability checks.\"\"\"\n",
    "    # Separate numerical and categorical columns\n",
    "    datetime_cols = ['datetime']\n",
    "    categorical_cols = ['symbol', 'instrument_type', 'market_regime', 'tech_regime', \n",
    "                       'volatility_regime', 'bear_regime', 'rates_regime']\n",
    "    numerical_cols = [col for col in data.columns \n",
    "                     if col not in datetime_cols + categorical_cols]\n",
    "    \n",
    "    # Use RobustScaler for better outlier handling\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    # Scale numerical features\n",
    "    numerical_data = pd.DataFrame(\n",
    "        scaler.fit_transform(data[numerical_cols].astype(float)),\n",
    "        columns=numerical_cols,\n",
    "        index=data.index\n",
    "    )\n",
    "    \n",
    "    # Process categorical features\n",
    "    categorical_data = pd.DataFrame(index=data.index)\n",
    "    for col in categorical_cols:\n",
    "        if col in data.columns:\n",
    "            # One-hot encode\n",
    "            dummies = pd.get_dummies(data[col], prefix=col)\n",
    "            categorical_data = pd.concat([categorical_data, dummies], axis=1)\n",
    "    \n",
    "    # Combine features\n",
    "    processed_data = pd.concat([numerical_data, categorical_data], axis=1)\n",
    "    \n",
    "    # Convert to float32 for better numerical stability\n",
    "    tensor_data = tf.cast(processed_data.values, tf.float32)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(tensor_data) - seq_length):\n",
    "        seq = tensor_data[i:(i + seq_length)]\n",
    "        target = tensor_data[i + seq_length]\n",
    "        \n",
    "        # Skip sequence if it contains any NaN or Inf\n",
    "        if tf.reduce_any(tf.math.is_nan(seq)) or tf.reduce_any(tf.math.is_inf(seq)):\n",
    "            continue\n",
    "            \n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    if not sequences:\n",
    "        raise ValueError(\"No valid sequences created\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X = tf.stack(sequences)\n",
    "    y = tf.stack(targets)\n",
    "    \n",
    "    # Calculate returns (using Close price index)\n",
    "    close_idx = numerical_cols.index('Close')\n",
    "    returns = (y[:, close_idx] - X[:, -1, close_idx]) / (X[:, -1, close_idx] + 1e-7)\n",
    "    \n",
    "    # Clip returns to prevent extreme values\n",
    "    returns = tf.clip_by_value(returns, -0.1, 0.1)\n",
    "    \n",
    "    # Create direction labels\n",
    "    directions = tf.cast(returns > 0, tf.float32)\n",
    "    \n",
    "    # Split train/val\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_val = X[:train_size], X[train_size:]\n",
    "    returns_train, returns_val = returns[:train_size], returns[train_size:]\n",
    "    directions_train, directions_val = directions[:train_size], directions[train_size:]\n",
    "    \n",
    "    # Create datasets with prefetch\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_train, {\n",
    "            'return_prediction': returns_train,\n",
    "            'direction_prediction': directions_train\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_val, {\n",
    "            'return_prediction': returns_val,\n",
    "            'direction_prediction': directions_val\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {len(sequences)} valid sequences\")\n",
    "    print(f\"Feature dimension: {X.shape[2]}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, scaler, X.shape[2]\n",
    "\n",
    "# Prepare data for training\n",
    "train_dataset, val_dataset, scaler, n_features = prepare_sequences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "class StabilityMonitorCallback(Callback):\n",
    "    \"\"\"Monitor numerical stability during training.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nan_count = 0\n",
    "        self.max_nans = 5  # Maximum number of NaN batches before stopping\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        # Check for NaN/Inf values\n",
    "        for metric, value in logs.items():\n",
    "            if np.isnan(value) or np.isinf(value):\n",
    "                self.nan_count += 1\n",
    "                print(f\"❌ {metric} has {value} at batch {batch}\")\n",
    "                if self.nan_count >= self.max_nans:\n",
    "                    print(\"Too many NaN values, stopping training\")\n",
    "                    self.model.stop_training = True\n",
    "                    return\n",
    "                \n",
    "                # Log recent values for debugging\n",
    "                if hasattr(self.model, 'history') and self.model.history is not None:\n",
    "                    print(f\"Recent values: {self.model.history.history.get(metric, [])[-5:]}\")\n",
    "        \n",
    "        # Reset counter if batch was good\n",
    "        if all(not (np.isnan(value) or np.isinf(value)) for value in logs.values()):\n",
    "            self.nan_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "def custom_return_loss(y_true, y_pred):\n",
    "    \"\"\"Stable return prediction loss.\"\"\"\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    # Ensure inputs are float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Clip values\n",
    "    y_pred = tf.clip_by_value(y_pred, -0.1, 0.1)\n",
    "    y_true = tf.clip_by_value(y_true, -0.1, 0.1)\n",
    "    \n",
    "    # Use Huber loss for robustness\n",
    "    huber = tf.keras.losses.Huber(delta=0.1)\n",
    "    loss = huber(y_true, y_pred)\n",
    "    \n",
    "    # Check for NaN/Inf\n",
    "    if tf.reduce_any(tf.math.is_nan(loss)) or tf.reduce_any(tf.math.is_inf(loss)):\n",
    "        tf.print(\"\\nWarning: NaN/Inf in return loss\", loss)\n",
    "        return tf.constant(0.1, dtype=tf.float32)  # Fallback value\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def custom_direction_loss(y_true, y_pred):\n",
    "    \"\"\"Stable direction prediction loss.\"\"\"\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    # Convert returns to direction and ensure float32\n",
    "    y_true_dir = tf.cast(y_true, tf.float32)  # Already binary\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Ensure predictions are between epsilon and 1-epsilon\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "    \n",
    "    # Binary cross entropy with stability\n",
    "    bce = -(y_true_dir * tf.math.log(y_pred + epsilon) +\n",
    "            (1 - y_true_dir) * tf.math.log(1 - y_pred + epsilon))\n",
    "    loss = tf.reduce_mean(bce)\n",
    "    \n",
    "    # Check for NaN/Inf\n",
    "    if tf.reduce_any(tf.math.is_nan(loss)) or tf.reduce_any(tf.math.is_inf(loss)):\n",
    "        tf.print(\"\\nWarning: NaN/Inf in direction loss\", loss)\n",
    "        return tf.constant(0.1, dtype=tf.float32)  # Fallback value\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "def build_model(seq_length, n_features):\n",
    "    \"\"\"Build model optimized for stability.\"\"\"\n",
    "    # Input layer with batch normalization\n",
    "    inputs = Input(shape=(seq_length, n_features))\n",
    "    x = BatchNormalization()(inputs)\n",
    "    \n",
    "    # Project input to match CNN output dimension\n",
    "    x_proj = Conv1D(128, 1, padding='same',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "    x_proj = LayerNormalization()(x_proj)\n",
    "    \n",
    "    # LSTM branch with layer normalization\n",
    "    lstm = LSTM(128, return_sequences=True, \n",
    "               recurrent_initializer='glorot_uniform',\n",
    "               kernel_regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "               recurrent_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    lstm = Dropout(0.2)(lstm)\n",
    "    \n",
    "    lstm2 = LSTM(128, return_sequences=True,\n",
    "                recurrent_initializer='glorot_uniform',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(1e-5),\n",
    "                recurrent_regularizer=tf.keras.regularizers.l2(1e-5))(lstm)\n",
    "    lstm2 = LayerNormalization()(lstm2)\n",
    "    lstm2 = Dropout(0.2)(lstm2)\n",
    "    \n",
    "    # CNN branch with residual connections\n",
    "    conv1 = Conv1D(128, 3, padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "    conv1 = LayerNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Add()([conv1, x_proj])  # Residual connection with projected input\n",
    "    \n",
    "    conv2 = Conv1D(128, 3, padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(1e-5))(conv1)\n",
    "    conv2 = LayerNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Add()([conv2, conv1])  # Residual connection\n",
    "    \n",
    "    # Multi-head attention\n",
    "    attention = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=8,\n",
    "        key_dim=32\n",
    "    )(lstm2, lstm2)\n",
    "    attention = LayerNormalization()(attention)\n",
    "    \n",
    "    # Combine branches\n",
    "    concat = Concatenate()([conv2, attention])\n",
    "    \n",
    "    # Global features\n",
    "    pooled = tf.keras.layers.GlobalAveragePooling1D()(concat)\n",
    "    \n",
    "    # Dense layers with batch normalization\n",
    "    dense1 = Dense(256, kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(1e-5))(pooled)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    dense1 = Activation('relu')(dense1)\n",
    "    dense1 = Dropout(0.3)(dense1)\n",
    "    \n",
    "    dense2 = Dense(128, kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(1e-5))(dense1)\n",
    "    dense2 = BatchNormalization()(dense2)\n",
    "    dense2 = Activation('relu')(dense2)\n",
    "    dense2 = Dropout(0.2)(dense2)\n",
    "    \n",
    "    # Output heads with stability\n",
    "    return_pred = Dense(1, name='return_prediction',\n",
    "                      kernel_initializer='glorot_normal',\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(1e-5))(dense2)\n",
    "    direction_pred = Dense(1, activation='sigmoid', name='direction_prediction',\n",
    "                         kernel_initializer='glorot_normal',\n",
    "                         kernel_regularizer=tf.keras.regularizers.l2(1e-5))(dense2)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[return_pred, direction_pred])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Build and compile model with stability improvements\n",
    "model = build_model(60, n_features)\n",
    "\n",
    "# Use a lower learning rate and gradient clipping\n",
    "optimizer = Adam(learning_rate=0.0001, clipnorm=0.5)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        'return_prediction': custom_return_loss,\n",
    "        'direction_prediction': custom_direction_loss\n",
    "    },\n",
    "    loss_weights={\n",
    "        'return_prediction': 0.3,\n",
    "        'direction_prediction': 0.7\n",
    "    },\n",
    "    metrics={\n",
    "        'return_prediction': [\n",
    "            tf.keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "            tf.keras.metrics.MeanSquaredError(name='mse')\n",
    "        ],\n",
    "        'direction_prediction': [\n",
    "            tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.5)\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train with stability improvements\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'deep_trading_model_a100.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,  # Increased patience\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    StabilityMonitorCallback(),\n",
    "    TensorBoard(\n",
    "        log_dir=f'./logs/{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "        histogram_freq=1,\n",
    "        update_freq='batch'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Use smaller batch size\n",
    "batch_size = 16  # Reduced from 32\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}