{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Learning Trading Model (Stability Optimized)\n",
                "\n",
                "This notebook implements a deep learning model for trading using:\n",
                "\n",
                "- LSTM + CNN + Attention architecture\n",
                "- Multi-symbol training with market regime awareness\n",
                "- Numerical stability improvements\n",
                "- Technical indicators and market context\n",
                "- Custom loss functions with stability\n",
                "\n",
                "# Install required packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "!pip install tensorflow pandas numpy scikit-learn yfinance plotly ta\n",
                "\n",
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import ta\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.layers import (\n",
                "    Input, LSTM, Dense, Dropout, BatchNormalization,\n",
                "    Conv1D, LayerNormalization, Activation, Add, Concatenate\n",
                ")\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau, Callback\n",
                "from tensorflow.keras.initializers import GlorotNormal, HeNormal\n",
                "from tensorflow.keras.regularizers import l2\n",
                "from sklearn.preprocessing import RobustScaler\n",
                "import yfinance as yf\n",
                "import plotly.graph_objects as go\n",
                "from datetime import datetime\n",
                "import pickle\n",
                "\n",
                "# Use float32 for better stability\n",
                "tf.keras.backend.set_floatx('float32')\n",
                "\n",
                "# Configure memory growth\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "if gpus:\n",
                "    for gpu in gpus:\n",
                "        tf.config.experimental.set_memory_growth(gpu, True)\n",
                "\n",
                "print(\"\\nGPU Configuration:\")\n",
                "print(\"Number of GPUs:\", len(gpus))\n",
                "print(\"Data Type:\", tf.keras.backend.floatx())\n",
                "print(\"TensorFlow version:\", tf.__version__)\n",
                "\n",
                "# Verify GPU is being used\n",
                "print(\"\\nGPU Available:\", tf.test.is_built_with_cuda())\n",
                "print(\"GPU Device:\", tf.test.gpu_device_name())"
            ]
        }
    ]
}

def prepare_sequences(data, seq_length=60):
    """Prepare sequences with stability checks."""
    # Separate numerical and categorical columns
    datetime_cols = ['datetime']
    categorical_cols = ['symbol', 'instrument_type', 'market_regime', 'tech_regime', 
                       'volatility_regime', 'bear_regime', 'rates_regime']
    numerical_cols = [col for col in data.columns 
                     if col not in datetime_cols + categorical_cols]
    
    # Use RobustScaler for better outlier handling
    scaler = RobustScaler()
    
    # Scale numerical features
    numerical_data = pd.DataFrame(
        scaler.fit_transform(data[numerical_cols].astype(float)),
        columns=numerical_cols,
        index=data.index
    )
    
    # Process categorical features
    categorical_data = pd.DataFrame(index=data.index)
    for col in categorical_cols:
        if col in data.columns:
            # One-hot encode
            dummies = pd.get_dummies(data[col], prefix=col)
            categorical_data = pd.concat([categorical_data, dummies], axis=1)
    
    # Combine features
    processed_data = pd.concat([numerical_data, categorical_data], axis=1)
    
    # Convert to float32 for better numerical stability
    tensor_data = tf.cast(processed_data.values, tf.float32)
    
    # Create sequences
    sequences = []
    targets = []
    
    for i in range(len(tensor_data) - seq_length):
        seq = tensor_data[i:(i + seq_length)]
        target = tensor_data[i + seq_length]
        
        # Skip sequence if it contains any NaN or Inf
        if tf.reduce_any(tf.math.is_nan(seq)) or tf.reduce_any(tf.math.is_inf(seq)):
            continue
            
        sequences.append(seq)
        targets.append(target)
    
    if not sequences:
        raise ValueError("No valid sequences created")
    
    # Convert to tensors
    X = tf.stack(sequences)
    y = tf.stack(targets)
    
    # Calculate returns (using Close price index)
    close_idx = numerical_cols.index('Close')
    returns = (y[:, close_idx] - X[:, -1, close_idx]) / (X[:, -1, close_idx] + 1e-7)
    
    # Clip returns to prevent extreme values
    returns = tf.clip_by_value(returns, -0.1, 0.1)
    
    # Create direction labels
    directions = tf.cast(returns > 0, tf.float32)
    
    # Split train/val
    train_size = int(len(X) * 0.8)
    X_train, X_val = X[:train_size], X[train_size:]
    returns_train, returns_val = returns[:train_size], returns[train_size:]
    directions_train, directions_val = directions[:train_size], directions[train_size:]
    
    # Create datasets with prefetch
    train_dataset = tf.data.Dataset.from_tensor_slices(
        (X_train, {
            'return_prediction': returns_train,
            'direction_prediction': directions_train
        })
    )
    
    val_dataset = tf.data.Dataset.from_tensor_slices(
        (X_val, {
            'return_prediction': returns_val,
            'direction_prediction': directions_val
        })
    )
    
    print(f"Created {len(sequences)} valid sequences")
    print(f"Feature dimension: {X.shape[2]}")
    
    return train_dataset, val_dataset, scaler, X.shape[2]

# Previous functions remain unchanged
# calculate_indicators
# calculate_cross_asset_features  
# calculate_market_regimes
# download_and_prepare_data
# StabilityMonitorCallback
# custom_return_loss
# custom_direction_loss
# build_model

# Prepare data for training
train_dataset, val_dataset, scaler, n_features = prepare_sequences(data)

# Build and compile model with stability improvements
model = build_model(60, n_features)

# Use a lower learning rate and gradient clipping
optimizer = Adam(learning_rate=0.0001, clipnorm=0.5)

model.compile(
    optimizer=optimizer,
    loss={
        'return_prediction': custom_return_loss,
        'direction_prediction': custom_direction_loss
    },
    loss_weights={
        'return_prediction': 0.3,
        'direction_prediction': 0.7
    },
    metrics={
        'return_prediction': [
            tf.keras.metrics.MeanAbsoluteError(name='mae'),
            tf.keras.metrics.MeanSquaredError(name='mse')
        ],
        'direction_prediction': [
            tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.5)
        ]
    }
)

model.summary()

# Train with stability improvements
callbacks = [
    ModelCheckpoint(
        'deep_trading_model_a100.keras',
        monitor='val_loss',
        save_best_only=True,
        mode='min',
        verbose=1
    ),
    EarlyStopping(
        monitor='val_loss',
        patience=15,  # Increased patience
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1
    ),
    StabilityMonitorCallback(),
    TensorBoard(
        log_dir=f'./logs/{datetime.now().strftime("%Y%m%d-%H%M%S")}',
        histogram_freq=1,
        update_freq='batch'
    )
]

    "    qqq_data = data['QQQ'].copy()\n",
    "    uvxy_data = data['UVXY'].copy()\n",
    "    sqqq_data = data['SQQQ'].copy()\n",
    "    tlt_data = data['TLT'].copy()\n",
    "    \n",
    "    # Market trend regime\n",
    "    spy_data['market_trend'] = spy_data['Close'].pct_change(20)\n",
    "    spy_data['market_regime'] = pd.qcut(\n",
    "        spy_data['market_trend'],\n",
    "        q=5,\n",
    "        labels=['strong_down', 'down', 'neutral', 'up', 'strong_up']\n",
    "    )\n",
    "    \n",
    "    # Tech sector regime\n",
    "    qqq_data['tech_trend'] = qqq_data['Close'].pct_change(20)\n",
    "    qqq_data['tech_regime'] = pd.qcut(\n",
    "        qqq_data['tech_trend'],\n",
    "        q=5,\n",
    "        labels=['tech_strong_down', 'tech_down', 'tech_neutral', 'tech_up', 'tech_strong_up']\n",
    "    )\n",
    "    \n",
    "    # Volatility regime\n",
    "    uvxy_data['volatility'] = uvxy_data['Close'].pct_change(5)\n",
    "    uvxy_data['volatility_regime'] = pd.qcut(\n",
    "        uvxy_data['volatility'],\n",
    "        q=5,\n",
    "        labels=['very_low', 'low', 'normal', 'high', 'very_high']\n",
    "    )\n",
    "    \n",
    "    # Bear market signals\n",
    "    sqqq_data['bear_signal'] = sqqq_data['Close'].pct_change(10)\n",
    "    sqqq_data['bear_regime'] = pd.qcut(\n",
    "        sqqq_data['bear_signal'],\n",
    "        q=5,\n",
    "        labels=['strong_bull', 'bull', 'neutral', 'bear', 'strong_bear']\n",
    "    )\n",
    "    \n",
    "    # Interest rate regime\n",
    "    tlt_data['rates_trend'] = tlt_data['Close'].pct_change(20)\n",
    "    tlt_data['rates_regime'] = pd.qcut(\n",
    "        tlt_data['rates_trend'],\n",
    "        q=5,\n",
    "        labels=['rates_up_strong', 'rates_up', 'rates_neutral', 'rates_down', 'rates_down_strong']\n",
    "    )\n",
    "    \n",
    "    # Additional market context features\n",
    "    context = {\n",
    "        'market_regime': spy_data['market_regime'],\n",
    "        'tech_regime': qqq_data['tech_regime'],\n",
    "        'volatility_regime': uvxy_data['volatility_regime'],\n",
    "        'bear_regime': sqqq_data['bear_regime'],\n",
    "        'rates_regime': tlt_data['rates_regime'],\n",
    "        \n",
    "        # Relative strength between markets\n",
    "        'tech_vs_spy': (qqq_data['Close'] / qqq_data['Close'].shift(20)) / \\\n",
    "                      (spy_data['Close'] / spy_data['Close'].shift(20)),\n",
    "                      \n",
    "        # Volatility trends\n",
    "        'vol_trend': uvxy_data['Close'].pct_change(5).rolling(5).mean(),\n",
    "        \n",
    "        # Market breadth\n",
    "        'market_momentum': spy_data['mom_20d'],\n",
    "        'tech_momentum': qqq_data['mom_20d'],\n",
    "        \n",
    "        # Risk measures\n",
    "        'market_risk': spy_data['atr_pct'].rolling(10).mean(),\n",
    "        'tech_risk': qqq_data['atr_pct'].rolling(10).mean()\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def download_and_prepare_data(tech_stocks, market_etfs, start_date='2023-01-01'):\n",
    "    \"\"\"Download and prepare data for multiple symbols with cross-asset relationships.\"\"\"\n",
    "    all_data = {}\n",
    "    all_symbols = tech_stocks + market_etfs\n",
    "    \n",
    "    # Download and process all data\n",
    "    for symbol in all_symbols:\n",
    "        print(f\"Processing {symbol}...\")\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        df = ticker.history(start=start_date)\n",
    "        df = calculate_indicators(df)\n",
    "        all_data[symbol] = df\n",
    "    \n",
    "    # Calculate cross-asset features for tech stocks\n",
    "    tech_data = {symbol: all_data[symbol] for symbol in tech_stocks}\n",
    "    cross_features = calculate_cross_asset_features(tech_data)\n",
    "    \n",
    "    # Calculate market regimes and context\n",
    "    market_context = calculate_market_regimes(all_data)\n",
    "    \n",
    "    # Combine all features\n",
    "    processed_data = []\n",
    "    for symbol in all_symbols:\n",
    "        df = all_data[symbol].copy()\n",
    "        \n",
    "        # Add instrument type\n",
    "        df['instrument_type'] = 'stock' if symbol in tech_stocks else 'etf'\n",
    "        df['symbol'] = symbol\n",
    "        \n",
    "        # Add cross-asset features for tech stocks\n",
    "        if symbol in tech_stocks:\n",
    "            df = pd.concat([df, cross_features[symbol]], axis=1)\n",
    "        \n",
    "        # Add market context\n",
    "        df = pd.concat([df, market_context], axis=1)\n",
    "        \n",
    "        processed_data.append(df)\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_data = pd.concat(processed_data, axis=0)\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "# Define instruments\n",
    "tech_stocks = ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'AMD', 'META']\n",
    "market_etfs = ['SPY', 'QQQ', 'UVXY', 'SQQQ', 'TLT']\n",
    "\n",
    "# Download and prepare data\n",
    "data = download_and_prepare_data(tech_stocks, market_etfs)\n",
    "\n",
    "# Plot sample data\n",
    "fig = go.Figure()\n",
    "for symbol in tech_stocks:\n",
    "    symbol_data = data[data['symbol'] == symbol]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=symbol_data.index,\n",
    "        y=symbol_data['Close'],\n",
    "        name=symbol\n",
    "    ))\n",
    "fig.update_layout(title='Tech Stock Prices', xaxis_title='Date', yaxis_title='Price')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare sequences for training\n",
    "def prepare_sequences(data, seq_length=60):\n",
    "    \"\"\"Prepare sequences for training.\"\"\"\n",
    "    # Drop non-numeric columns\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(numeric_data)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y_return, y_direction = [], [], []\n",
    "    for i in range(len(scaled_data) - seq_length):\n",
    "        X.append(scaled_data[i:(i + seq_length)])\n",
    "        next_return = scaled_data[i + seq_length, numeric_data.columns.get_loc('Close')]\n",
    "        y_return.append([next_return])  # Make it 2D\n",
    "        y_direction.append([1.0 if next_return > 0 else 0.0])  # Make it 2D\n",
    "    \n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y_return = np.array(y_return, dtype=np.float32)\n",
    "    y_direction = np.array(y_direction, dtype=np.float32)\n",
    "    \n",
    "    # Split into train/val\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train, X_val = X[:split], X[split:]\n",
    "    y_train_return, y_val_return = y_return[:split], y_return[split:]\n",
    "    y_train_direction, y_val_direction = y_direction[:split], y_direction[split:]\n",
    "    \n",
    "    # Create TF datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_train, {'return_prediction': y_train_return, 'direction_prediction': y_train_direction})\n",
    "    )\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_val, {'return_prediction': y_val_return, 'direction_prediction': y_val_direction})\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, scaler, X_train.shape[2]\n",
    "\n",
    "# Prepare data for training\n",
    "train_dataset, val_dataset, scaler, n_features = prepare_sequences(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Trading Model (Stability Optimized)\n",
    "\n",
    "This notebook implements a deep learning model for trading using:\n",
    "\n",
    "- LSTM + CNN + Attention architecture\n",
    "- Multi-symbol training with market regime awareness\n",
    "- Numerical stability improvements\n",
    "- Technical indicators and market context\n",
    "- Custom loss functions with stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow pandas numpy scikit-learn yfinance plotly ta\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ta\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Dense, Dropout, BatchNormalization,\n",
    "    Conv1D, LayerNormalization, Activation, Add, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.initializers import GlorotNormal, HeNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import yfinance as yf\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# Use float32 for better stability\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "# Configure memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(\"\\nGPU Configuration:\")\n",
    "print(\"Number of GPUs:\", len(gpus))\n",
    "print(\"Data Type:\", tf.keras.backend.floatx())\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Verify GPU is being used\n",
    "print(\"\\nGPU Available:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU Device:\", tf.test.gpu_device_name())"
   ]
},
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prepare_sequences(data, seq_length=60):\n",
    "    \"\"\"Prepare sequences with stability checks.\"\"\"\n",
    "    # Separate numerical and categorical columns\n",
    "    datetime_cols = ['datetime']\n",
    "    categorical_cols = ['symbol', 'instrument_type', 'market_regime', 'tech_regime', \n",
    "                       'volatility_regime', 'bear_regime', 'rates_regime']\n",
    "    numerical_cols = [col for col in data.columns \n",
    "                     if col not in datetime_cols + categorical_cols]\n",
    "    \n",
    "    # Use RobustScaler for better outlier handling\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    # Scale numerical features\n",
    "    numerical_data = pd.DataFrame(\n",
    "        scaler.fit_transform(data[numerical_cols].astype(float)),\n",
    "        columns=numerical_cols,\n",
    "        index=data.index\n",
    "    )\n",
    "    \n",
    "    # Process categorical features\n",
    "    categorical_data = pd.DataFrame(index=data.index)\n",
    "    for col in categorical_cols:\n",
    "        if col in data.columns:\n",
    "            # One-hot encode\n",
    "            dummies = pd.get_dummies(data[col], prefix=col)\n",
    "            categorical_data = pd.concat([categorical_data, dummies], axis=1)\n",
    "    \n",
    "    # Combine features\n",
    "    processed_data = pd.concat([numerical_data, categorical_data], axis=1)\n",
    "    \n",
    "    # Convert to float32 for better numerical stability\n",
    "    tensor_data = tf.cast(processed_data.values, tf.float32)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(tensor_data) - seq_length):\n",
    "        seq = tensor_data[i:(i + seq_length)]\n",
    "        target = tensor_data[i + seq_length]\n",
    "        \n",
    "        # Skip sequence if it contains any NaN or Inf\n",
    "        if tf.reduce_any(tf.math.is_nan(seq)) or tf.reduce_any(tf.math.is_inf(seq)):\n",
    "            continue\n",
    "            \n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    if not sequences:\n",
    "        raise ValueError(\"No valid sequences created\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X = tf.stack(sequences)\n",
    "    y = tf.stack(targets)\n",
    "    \n",
    "    # Calculate returns (using Close price index)\n",
    "    close_idx = numerical_cols.index('Close')\n",
    "    returns = (y[:, close_idx] - X[:, -1, close_idx]) / (X[:, -1, close_idx] + 1e-7)\n",
    "    \n",
    "    # Clip returns to prevent extreme values\n",
    "    returns = tf.clip_by_value(returns, -0.1, 0.1)\n",
    "    \n",
    "    # Create direction labels\n",
    "    directions = tf.cast(returns > 0, tf.float32)\n",
    "    \n",
    "    # Split train/val\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_val = X[:train_size], X[train_size:]\n",
    "    returns_train, returns_val = returns[:train_size], returns[train_size:]\n",
    "    directions_train, directions_val = directions[:train_size], directions[train_size:]\n",
    "    \n",
    "    # Create datasets with prefetch\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_train, {\n",
    "            'return_prediction': returns_train,\n",
    "            'direction_prediction': directions_train\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (X_val, {\n",
    "            'return_prediction': returns_val,\n",
    "            'direction_prediction': directions_val\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {len(sequences)} valid sequences\")\n",
    "    print(f\"Feature dimension: {X.shape[2]}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, scaler, X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Previous functions remain unchanged\n",
    "# calculate_indicators\n",
    "# calculate_cross_asset_features  \n",
    "# calculate_market_regimes\n",
    "# download_and_prepare_data\n",
    "# StabilityMonitorCallback\n",
    "# custom_return_loss\n",
    "# custom_direction_loss\n",
    "# build_model\n",
    "\n",
    "# Define instruments\n",
    "tech_stocks = ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'AMD', 'META']\n",
    "market_etfs = ['SPY', 'QQQ', 'UVXY', 'SQQQ', 'TLT']\n",
    "\n",
    "# Download and prepare data\n",
    "data = download_and_prepare_data(tech_stocks, market_etfs)\n",
    "\n",
    "# Prepare data for training\n",
    "train_dataset, val_dataset, scaler, n_features = prepare_sequences(data)\n",
    "\n",
    "# Build and compile model with stability improvements\n",
    "model = build_model(60, n_features)\n",
    "\n",
    "# Use a lower learning rate and gradient clipping\n",
    "optimizer = Adam(learning_rate=0.0001, clipnorm=0.5)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        'return_prediction': custom_return_loss,\n",
    "        'direction_prediction': custom_direction_loss\n",
    "    },\n",
    "    loss_weights={\n",
    "        'return_prediction': 0.3,\n",
    "        'direction_prediction': 0.7\n",
    "    },\n",
    "    metrics={\n",
    "        'return_prediction': [\n",
    "            tf.keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "            tf.keras.metrics.MeanSquaredError(name='mse')\n",
    "        ],\n",
    "        'direction_prediction': [\n",
    "            tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.5)\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train with stability improvements\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'deep_trading_model_a100.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,  # Increased patience\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    StabilityMonitorCallback(),\n",
    "    TensorBoard(\n",
    "        log_dir=f'./logs/{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "        histogram_freq=1,\n",
    "        update_freq='batch'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Use smaller batch size\n",
    "batch_size = 16  # Reduced from 32\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
