,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Stability Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class StabilityMonitorCallback(Callback):\n",
    "    \"\"\"Monitor numerical stability during training.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.nan_count = 0\n",
    "        self.max_nans = 5  # Maximum number of NaN batches before stopping\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        # Check for NaN/Inf values\n",
    "        for metric, value in logs.items():\n",
    "            if np.isnan(value) or np.isinf(value):\n",
    "                self.nan_count += 1\n",
    "                print(f\"❌ {metric} has {value} at batch {batch}\")\n",
    "                if self.nan_count >= self.max_nans:\n",
    "                    print(\"Too many NaN values, stopping training\")\n",
    "                    self.model.stop_training = True\n",
    "                    return\n",
    "                \n",
    "                # Log recent values for debugging\n",
    "                if hasattr(self.model, 'history') and self.model.history is not None:\n",
    "                    print(f\"Recent values: {self.model.history.history.get(metric, [])[-5:]}\")\n",
    "        \n",
    "        # Reset counter if batch was good\n",
    "        if all(not (np.isnan(value) or np.isinf(value)) for value in logs.values()):\n",
    "            self.nan_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@tf.function\n",
    "def custom_return_loss(y_true, y_pred):\n",
    "    \"\"\"Stable return prediction loss.\"\"\"\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    # Ensure inputs are float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Clip values\n",
    "    y_pred = tf.clip_by_value(y_pred, -0.1, 0.1)\n",
    "    y_true = tf.clip_by_value(y_true, -0.1, 0.1)\n",
    "    \n",
    "    # Use Huber loss for robustness\n",
    "    huber = tf.keras.losses.Huber(delta=0.1)\n",
    "    loss = huber(y_true, y_pred)\n",
    "    \n",
    "    # Check for NaN/Inf\n",
    "    if tf.reduce_any(tf.math.is_nan(loss)) or tf.reduce_any(tf.math.is_inf(loss)):\n",
    "        tf.print(\"\\nWarning: NaN/Inf in return loss\", loss)\n",
    "        return tf.constant(0.1, dtype=tf.float32)  # Fallback value\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def custom_direction_loss(y_true, y_pred):\n",
    "    \"\"\"Stable direction prediction loss.\"\"\"\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    # Convert returns to direction and ensure float32\n",
    "    y_true_dir = tf.cast(y_true > 0, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    # Ensure predictions are between epsilon and 1-epsilon\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "    \n",
    "    # Binary cross entropy with stability\n",
    "    bce = -(y_true_dir * tf.math.log(y_pred + epsilon) +\n",
    "            (1 - y_true_dir) * tf.math.log(1 - y_pred + epsilon))\n",
    "    loss = tf.reduce_mean(bce)\n",
    "    \n",
    "    # Check for NaN/Inf\n",
    "    if tf.reduce_any(tf.math.is_nan(loss)) or tf.reduce_any(tf.math.is_inf(loss)):\n",
    "        tf.print(\"\\nWarning: NaN/Inf in direction loss\", loss)\n",
    "        return tf.constant(0.1, dtype=tf.float32)  # Fallback value\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def build_model(seq_length, n_features):\n",
    "    \"\"\"Build model optimized for A100 GPU.\"\"\"\n",
    "    # Input layer with batch normalization\n",
    "    inputs = Input(shape=(seq_length, n_features))\n",
    "    x = BatchNormalization()(inputs)\n",
    "    \n",
    "    # LSTM branch with layer normalization\n",
    "    lstm = LSTM(128, return_sequences=True, \n",
    "               recurrent_initializer='glorot_uniform',  # More stable initialization\n",
    "               kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)  # L2 regularization\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    lstm = Dropout(0.2)(lstm)\n",
    "    \n",
    "    lstm2 = LSTM(128, return_sequences=True,\n",
    "                recurrent_initializer='glorot_uniform',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(1e-5))(lstm)\n",
    "    lstm2 = LayerNormalization()(lstm2)\n",
    "    lstm2 = Dropout(0.2)(lstm2)\n",
    "    \n",
    "    # CNN branch with residual connections\n",
    "    conv1 = Conv1D(128, 3, padding='same',\n",
    "                  kernel_initializer='he_normal',  # Better for ReLU\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(1e-5))(x)\n",
    "    conv1 = LayerNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Add()([conv1, x])  # Residual connection\n",
    "    \n",
    "    conv2 = Conv1D(128, 3, padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(1e-5))(conv1)\n",
    "    conv2 = LayerNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Add()([conv2, conv1])  # Residual connection\n",
    "    \n",
    "    # Multi-head attention\n",
    "    attention = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=8,\n",
    "        key_dim=32\n",
    "    )(lstm2, lstm2)\n",
    "    attention = LayerNormalization()(attention)\n",
    "    \n",
    "    # Combine branches\n",
    "    concat = Concatenate()([conv2, attention])\n",
    "    \n",
    "    # Global features\n",
    "    pooled = tf.keras.layers.GlobalAveragePooling1D()(concat)\n",
    "    \n",
    "    # Dense layers with batch normalization\n",
    "    dense1 = Dense(256, kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(1e-5))(pooled)\n",
    "    dense1 = BatchNormalization()(dense1)\n",
    "    dense1 = Activation('relu')(dense1)\n",
    "    dense1 = Dropout(0.3)(dense1)\n",
    "    \n",
    "    dense2 = Dense(128, kernel_initializer='he_normal',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(1e-5))(dense1)\n",
    "    dense2 = BatchNormalization()(dense2)\n",
    "    dense2 = Activation('relu')(dense2)\n",
    "    dense2 = Dropout(0.2)(dense2)\n",
    "    \n",
    "    # Output heads with stability\n",
    "    return_pred = Dense(1, name='return_prediction',\n",
    "                      kernel_initializer='glorot_normal',\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(1e-5))(dense2)\n",
    "    direction_pred = Dense(1, activation='sigmoid', name='direction_prediction',\n",
    "                         kernel_initializer='glorot_normal',\n",
    "                         kernel_regularizer=tf.keras.regularizers.l2(1e-5))(dense2)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[return_pred, direction_pred])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Build and compile model with stability improvements\n",
    "model = build_model(60, n_features)\n",
    "\n",
    "# Use a lower learning rate and gradient clipping\n",
    "base_optimizer = Adam(learning_rate=0.0001)\n",
    "base_optimizer.clipnorm = 0.5\n",
    "optimizer = tf.keras.mixed_precision.LossScaleOptimizer(base_optimizer)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        'return_prediction': custom_return_loss,\n",
    "        'direction_prediction': custom_direction_loss\n",
    "    },\n",
    "    loss_weights={\n",
    "        'return_prediction': 0.3,\n",
    "        'direction_prediction': 0.7\n",
    "    },\n",
    "    metrics={\n",
    "        'return_prediction': [\n",
    "            tf.keras.metrics.MeanAbsoluteError(name='mae'),\n",
    "            tf.keras.metrics.MeanSquaredError(name='mse')\n",
    "        ],\n",
    "        'direction_prediction': [\n",
    "            tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.5)\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train with stability improvements\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'deep_trading_model_a100.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,  # Increased patience\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    StabilityMonitorCallback(),\n",
    "    TensorBoard(\n",
    "        log_dir=f'./logs/{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "        histogram_freq=1,\n",
    "        update_freq='batch'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Use smaller batch size\n",
    "batch_size = 16  # Reduced from 32\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ]
}
